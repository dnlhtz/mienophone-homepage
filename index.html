<!DOCTYPE html><!--  This site was created in Webflow. http://www.webflow.com  -->
<!--  Last Published: Fri May 21 2021 14:13:37 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="609d709c96e4006424893943" data-wf-site="609d709c96e400c15f893942">
<head>
  <meta charset="utf-8">
  <title>Mienophone</title>
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/mienophone.webflow.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["IBM Plex Mono:100,regular,500"]  }});</script>
  <!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] -->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
</head>
<body class="body">
  <div class="head"></div>
  <div class="content">
    <div class="w-container">
      <h1 class="heading">Mienophone</h1>
      <div class="regular mb-100">
        <a href="https://zhewang.art/" target="_blank">Zhé Wang</a> &amp; <a href="http://dnlhtz.com/" target="_blank">Daniel Heitz</a>
      </div>
      <div class="large"><strong>21.–25. Mai 2021</strong></div>
      <div class="regular mb-20">
        <a href="https://www.migrantbirdspace.com/" target="_blank">Migrant Bird Space</a>, Berlin
      </div><img src="images/migrantbird.png" loading="lazy" width="150" sizes="(max-width: 479px) 140px, 150px" srcset="images/migrantbird-p-500.png 500w, images/migrantbird.png 600w" alt="" class="location">
      <div class="small mb-20 mt-50">Funded by</div>
      <a href="http://www.karin-abt-straubinger-stiftung.de/" target="_blank" class="w-inline-block"><img src="images/sponsor.png" loading="lazy" width="300" sizes="(max-width: 479px) 83vw, 300px" srcset="images/sponsor-p-500.png 500w, images/sponsor.png 600w" alt="" class="sponsor"></a>
    </div>
  </div>
  <div class="content">
    <div class="w-container">
      <div class="text">Mienophone is created by <a href="https://zhewang.art/" target="_blank"><strong>Zhe Wang</strong></a> and <strong>Daniel Heitz</strong> in collaboration, since 2019.<br><br>Music is controlling our emotions. It sometimes feels a bit like being a string puppet, while music is the puppet master. It can make us dance or sink into sadness. <br>As Machine Learning Algorithm is becoming better at reading human emotions, can we use this technology to reverse this power of control? How does music sound like if it was directly controlled by our expressions and not the other way around? How would we react? Would it perhaps reinforce the emotion? Can technology help us to become more aware and better understand our emotions? And how does all this challenge the purpose of art and the role of the artist in the future? These are the questions the artists explore with the Mienophone. In the exhibition, the artist duo is having a dialogue through different media: the Mienophone installation, video works and photos.<br>‍<br>The project is funded by <a href="http://www.karin-abt-straubinger-stiftung.de/">KARIN ABT-STRAUBINGER Stiftung<br>‍</a>Technical support: <a href="https://www.berlinglassworks.com/">Berlin Glassworks</a><br><br><strong>About &quot;Mienophone&quot;</strong><br>Mienophone is an installation consisting of glass sculpture and a software synthesizer developed in MAX, a visual programming language for music and multimedia. A camera is recording a live video stream of the viewer. Selected frames of the video live stream are transmitted to the Microsoft Cognitive Services Vision API. The Machine Learning Algorithm returns a numeric value for a set of emotions detected in the player&#x27;s face in the video. The emotions detected are anger, contempt, disgust, fear, happiness, neutral, sadness, and surprise. Each value is used to control a set of parameters of the software synthesizer and allows the viewer to influence the sound by expressing emotions. <br>‍<br>The camera is placed in an organic-shaped glass sculpture. Since the glass surface is not flat, the camera&#x27;s line of sight will be distorted, so the Machine Learning Algorithm will also read the viewer&#x27;s image distorted. This also reminds us that whether Machine Learning Algorithm can see us clearly, does AI and humans really see each other? Is the &quot;seeing&quot; equal? The shape of the glass is symbolically presented in a form similar to a human face; just like our imagination of AI, it is a human-like, inhuman subject. When faced with this infant-sized, fragile, human-like device, we will regard ourselves as parents of this new intelligence.The main element that composes glass is silicide, and computer chips are the same. The history of glassmaking can be traced back to BC, far before the invention of iron melting technology. Although computer chips have only be invented for a few decades, the powerful driving force they provide to the development of human society cannot be quantified.<br>The development of Machine Learning Algorithm has been closely linked to the future of humankind. In the most optimistic scenario, we may be able to create a utopia where humans and superintelligence can coexist peacefully. Or, the omnipotent AI has mastered higher intelligence than human beings. Human beings live in a society controlled by AI, lose their creative ability, and only lament their own destiny.<br><br><strong>Videos</strong><br>In &quot;once I showed you what I want,&quot; Zhé deconstructed the performances, images, and emotions of actors in movies by reinterpreting the close-ups of performers in old movies. Emotions are expressed through facial expressions, often in a moment, but at this moment, tens of thousands of facial nerves drive muscles to move together, just like a feast. Sometimes the emotions expressed by people are also extremely complex, such as self-deprecating laughter under extreme sadness. Can AI read such complex emotions instead of just telling us that this expression is 99.9% happy?<br><br>In &quot;let&#x27;s laugh together,&quot; Zhé focused on the expression of laughter. In <strong><em>Dialectic of Enlightenment </em></strong>Adorno&#x27;s description of laughter is very touché,<br>‍<br><span><em>&quot;Conciliatory laughter is heard as the echo of an escape from power; the wrong kind overcomes fear by capitulating to the forces which are to be feared. It is the echo of power as something inescapable.&quot; <br>‍</em></span><em><br></em>In our power interaction with others, laughter is produced. We laugh at our weaknesses, we laugh at our ignorance, we laugh at the laughs of others.<br><br><br><br></div>
    </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=609d709c96e400c15f893942" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script>
  <!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] -->
</body>
</html>